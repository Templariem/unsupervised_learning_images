# unsupervised_learning_images
Un recopilatorio de papers y técnicas de aprendizaje no supervisado aplicado a imágenes considerando:

1. Basic unsupervised learning
2. Clusters algorithms
3. Semantic image Segmentation (SiS)
4. Unsupervised SiS
5. SAM
6. DINO
7. GroundingDINO

| Title | Description | Tag | Date | Code |
|-------|-------------|-----|------|--------|
|[SCAN: Learning to Classify Images without Labels](https://arxiv.org/abs/2005.12320)|Can we automatically group images into semantically meaningful clusters when ground-truth annotations are absent? The task of unsupervised image classification remains an important, and open challenge in computer vision. Several recent approaches have tried to tackle this problem in an end-to-end fashion. In this paper, we deviate from recent works, and advocate a two-step approach where feature learning and clustering are decoupled. First, a self-supervised task from representation learning is employed to obtain semantically meaningful features. Second, we use the obtained features as a prior in a learnable clustering approach. In doing so, we remove the ability for cluster learning to depend on low-level features, which is present in current end-to-end learning approaches. Experimental evaluation shows that we outperform state-of-the-art methods by large margins, in particular +26.6% on CIFAR10, +25.0% on CIFAR100-20 and +21.3% on STL10 in terms of classification accuracy. Furthermore, our method is the first to perform well on a large-scale dataset for image classification. In particular, we obtain promising results on ImageNet, and outperform several semi-supervised learning methods in the low-data regime without the use of any ground-truth annotations.|Unsupervised Clasification|2020|[Github](https://github.com/wvangansbeke/Unsupervised-Classification)|
|[Semantic Image Segmentation: Two Decades of Research](https://arxiv.org/abs/2302.06378)|Semantic image segmentation (SiS) plays a fundamental role in a broad variety of computer vision applications, providing key information for the global understanding of an image. This survey is an effort to summarize two decades of research in the field of SiS, where we propose a literature review of solutions starting from early historical methods followed by an overview of more recent deep learning methods including the latest trend of using transformers. We complement the review by discussing particular cases of the weak supervision and side machine learning techniques that can be used to improve the semantic segmentation such as curriculum, incremental or self-supervised learning. State-of-the-art SiS models rely on a large amount of annotated samples, which are more expensive to obtain than labels for tasks such as image classification. Since unlabeled data is instead significantly cheaper to obtain, it is not surprising that Unsupervised Domain Adaptation (UDA) reached a broad success within the semantic segmentation community. Therefore, a second core contribution of this book is to summarize five years of a rapidly growing field, Domain Adaptation for Semantic Image Segmentation (DASiS) which embraces the importance of semantic segmentation itself and a critical need of adapting segmentation models to new environments. In addition to providing a comprehensive survey on DASiS techniques, we unveil also newer trends such as multi-domain learning, domain generalization, domain incremental learning, test-time adaptation and source-free domain adaptation. Finally, we conclude this survey by describing datasets and benchmarks most widely used in SiS and DASiS and briefly discuss related tasks such as instance and panoptic image segmentation, as well as applications such as medical image segmentation. | Semantic Segmentation | 2023 | - |
|[Few Shot Semantic Segmentation](https://github.com/xiaomengyc/Few-Shot-Semantic-Segmentation-Papers/blob/master/README.md)|Recopilatorio de papers asociados a la técnica de "Few Shot Semantic Segmentation" el cual consiste en entrenar un modelo no supervisado de segmentación considerando un conjunto menor de imagenes etiquetadas|Few Shot Semantic Segmentation|2017-2023| [Github](https://github.com/xiaomengyc/Few-Shot-Semantic-Segmentation-Papers/blob/master/README.md) |
|[Weakly Semantic Segmentation](https://github.com/PengtaoJiang/Awesome-Weakly-Supervised-Semantic-Segmentation-Papers)|Recopilatorio de papers asociados a "Weakly Semantic Segmentation" el cual consiste en entrenar un modelo de segmentación considerando una baja densidad de etiquetas por imagen|Weakly Semantic Segmentation|2017-2023| [Github](https://github.com/PengtaoJiang/Awesome-Weakly-Supervised-Semantic-Segmentation-Papers)|
|[Unsupervised Semantic Segmentation by Contrasting Object Mask Proposals](https://arxiv.org/pdf/2102.06191)|Being able to learn dense semantic representations of images without supervision is an important problem in com- puter vision. However, despite its significance, this problem remains rather unexplored, with a few exceptions that con- sidered unsupervised semantic segmentation on small-scale datasets with a narrow visual domain. In this paper, we make a first attempt to tackle the problem on datasets that have been traditionally utilized for the supervised case. To achieve this, we introduce a two-step framework that adopts a predetermined mid-level prior in a contrastive optimiza- tion objective to learn pixel embeddings. This marks a large deviation from existing works that relied on proxy tasks or end-to-end clustering. Additionally, we argue about the im- portance of having a prior that contains information about objects, or their parts, and discuss several possibilities to obtain such a prior in an unsupervised manner. Experimental evaluation shows that our method comes with key advantages over existing works. First, the learned pixel embeddings can be directly clustered in semantic groups using K-Means on PASCAL. Under the fully unsu- pervised setting, there is no precedent in solving the se- mantic segmentation task on such a challenging benchmark. Second, our representations can improve over strong base- lines when transferred to new datasets, e.g. COCO and DAVIS. The code is available.|Unsupervised Semantic Segmentation|2021|[Github](https://github.com/wvangansbeke/Unsupervised-Semantic-Segmentation)|
|[Segment Anything Model (SAM)](https://github.com/baibizhe/Awesome-SAM)|Recopilatorio de artículos con códigos de SAM en diversas tareas|Detections, Segmentations, Fine-tunning|-|[Github](https://github.com/baibizhe/Awesome-SAM)|
|[Unsupervised feature extraction of aerial images for clustering and understanding hazardous road segments](https://www.nature.com/articles/s41598-023-38100-1)|Aerial image data are becoming more widely available, and analysis techniques based on supervised learning are advancing their use in a wide variety of remote sensing contexts. However, supervised learning requires training datasets which are not always available or easy to construct with aerial imagery. In this respect, unsupervised machine learning techniques present important advantages. This work presents a novel pipeline to demonstrate how available aerial imagery can be used to better the provision of services related to the built environment, using the case study of road traffic collisions (RTCs) across three cities in the UK. In this paper, we show how aerial imagery can be leveraged to extract latent features of the built environment from the purely visual representation of top-down images. With these latent image features in hand to represent the urban structure, this work then demonstrates how hazardous road segments can be clustered to provide a data-augmented aid for road safety experts to enhance their nuanced understanding of how and where different types of RTCs occur.| Unsupervised Segmentation | 2023 | - |
|[Imagenes satelitales](https://github.com/satellite-image-deep-learning/techniques)| Recopilatorio de papers utilizando deep learning en imagenes satelitales en diversas tareas como detección, segmentación, analisis temporal, etc.| Imagenes satelitales | - | [Github](https://github.com/satellite-image-deep-learning/techniques) |
|[ProFeat: Unsupervised image clustering via progressive feature refinement](https://www.sciencedirect.com/science/article/pii/S0167865522003282)|Unsupervised image clustering is a chicken-and-egg problem that involves representation learning and clustering. To resolve the inter-dependency between them, many approaches that iteratively perform the two tasks have been proposed, but their accuracy is limited due to inaccurate intermediate representations and clusters. To overcome this, this paper proposes ProFeat , a novel iterative approach to unsupervised image clustering based on progressive feature refinement. To learn discriminative features for clustering while avoiding adversarial influence from inaccurate intermediate clusters, ProFeat rigorously divides representation learning and clustering by modeling a neural network for clustering as a composition of an embedding and a clustering function and introducing an auxiliary embedding function. ProFeat progressively refines representations using confident samples from intermediate clusters using an extended contrastive loss. This paper also proposes ensemble-based feature refinement for more robust clustering. Our experiments demonstrate that ProFeat achieves superior results compared to previous methods.|Unsupervised Segmentation|2022|-|
|[A Rapid Review of Clustering Algorithms](https://arxiv.org/abs/2401.07389)| Clustering algorithms aim to organize data into groups or clusters based on the inherent patterns and similarities within the data. They play an important role in today's life, such as in marketing and e-commerce, healthcare, data organization and analysis, and social media. Numerous clustering algorithms exist, with ongoing developments introducing new ones. Each algorithm possesses its own set of strengths and weaknesses, and as of now, there is no universally applicable algorithm for all tasks. In this work, we analyzed existing clustering algorithms and classify mainstream algorithms across five different dimensions: underlying principles and characteristics, data point assignment to clusters, dataset capacity, predefined cluster numbers and application area. This classification facilitates researchers in understanding clustering algorithms from various perspectives and helps them identify algorithms suitable for solving specific tasks. Finally, we discussed the current trends and potential future directions in clustering algorithms. We also identified and discussed open challenges and unresolved issues in the field. | Clustering Algorithms | 2024 | - |
| [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929) | While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train. | ViT transformers | 2020 | [Github](https://github.com/lucidrains/vit-pytorch) |
| [Emerging Properties in Self-Supervised Vision Transformers](https://arxiv.org/abs/2104.14294) | In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers, reaching 78.3% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder, multi-crop training, and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1% top-1 on ImageNet in linear evaluation with ViT-Base. | DINO | 2021 | [Github](https://github.com/facebookresearch/dino) |
| [DINOv2: Learning Robust Visual Features without Supervision](https://arxiv.org/abs/2304.07193) | The recent breakthroughs in natural language processing for model pretraining on large quantities of data have opened the way for similar foundation models in computer vision. These models could greatly simplify the use of images in any system by producing all-purpose visual features, i.e., features that work across image distributions and tasks without finetuning. This work shows that existing pretraining methods, especially self-supervised methods, can produce such features if trained on enough curated data from diverse sources. We revisit existing approaches and combine different techniques to scale our pretraining in terms of data and model size. Most of the technical contributions aim at accelerating and stabilizing the training at scale. In terms of data, we propose an automatic pipeline to build a dedicated, diverse, and curated image dataset instead of uncurated data, as typically done in the self-supervised literature. In terms of models, we train a ViT model (Dosovitskiy et al., 2020) with 1B parameters and distill it into a series of smaller models that surpass the best available all-purpose features, OpenCLIP (Ilharco et al., 2021) on most of the benchmarks at image and pixel levels. | DINOv2 | 2024 | [Github](https://github.com/facebookresearch/dinov2?tab=readme-ov-file) |
| [Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection](https://arxiv.org/abs/2303.05499) | In this paper, we present an open-set object detector, called Grounding DINO, by marrying Transformer-based detector DINO with grounded pre-training, which can detect arbitrary objects with human inputs such as category names or referring expressions. The key solution of open-set object detection is introducing language to a closed-set detector for open-set concept generalization. To effectively fuse language and vision modalities, we conceptually divide a closed-set detector into three phases and propose a tight fusion solution, which includes a feature enhancer, a language-guided query selection, and a cross-modality decoder for cross-modality fusion. While previous works mainly evaluate open-set object detection on novel categories, we propose to also perform evaluations on referring expression comprehension for objects specified with attributes. Grounding DINO performs remarkably well on all three settings, including benchmarks on COCO, LVIS, ODinW, and RefCOCO/+/g. Grounding DINO achieves a 52.5 AP on the COCO detection zero-shot transfer benchmark, i.e., without any training data from COCO. It sets a new record on the ODinW zero-shot benchmark with a mean 26.1 AP. Code will be available at \url{this https URL}. | GroundingDINO | 2023 | [Github](https://github.com/IDEA-Research/GroundingDINO) |
